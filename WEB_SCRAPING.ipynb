{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CODE FOR WEB SCRAPING\n"
      ],
      "metadata": {
        "id": "RgRUNuWM_Hbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2cyeWRw-A2p",
        "outputId": "b73bb20e-c856-45ff-9df6-e38aedb98b47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.26.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVHps6PW9kCY",
        "outputId": "48f2edb9-5295-4b8e-8649-abc09d24ccd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed and saved to amazon_products.csv\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Initialize Selenium WebDriver (you may need to specify the path to your chromedriver)\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')  # Run in headless mode\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://www.amazon.in/s?rh=n%3A6612025031&fs=true&ref=lp_6612025031_sar\"\n",
        "driver.get(url)\n",
        "time.sleep(3)  # Initial sleep to load the page\n",
        "\n",
        "# Scroll to load all products (Amazon may use lazy loading)\n",
        "scroll_pause_time = 2  # Time to wait after scrolling\n",
        "screen_height = driver.execute_script(\"return window.screen.height;\")  # Get screen height\n",
        "scrolls = 3  # Number of scrolls, increase if you need more data\n",
        "\n",
        "for _ in range(scrolls):\n",
        "    driver.execute_script(\"window.scrollTo(0, window.scrollY + {});\".format(screen_height))\n",
        "    time.sleep(scroll_pause_time)\n",
        "\n",
        "# Extract page source and parse with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "driver.quit()\n",
        "\n",
        "# Lists to store the scraped data\n",
        "product_names = []\n",
        "prices = []\n",
        "ratings = []\n",
        "sellers = []\n",
        "\n",
        "# Find all product containers\n",
        "products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "# Loop through each product and extract details\n",
        "for product in products:\n",
        "    # Extract product name\n",
        "    name = product.h2.text.strip() if product.h2 else 'N/A'\n",
        "    product_names.append(name)\n",
        "\n",
        "    # Extract price\n",
        "    price = product.find('span', 'a-price-whole')\n",
        "    if price:\n",
        "        price = price.text.strip()\n",
        "    else:\n",
        "        price = 'N/A'\n",
        "    prices.append(price)\n",
        "\n",
        "    # Extract rating\n",
        "    rating = product.find('span', 'a-icon-alt')\n",
        "    if rating:\n",
        "        rating = rating.text.strip().split()[0]  # Get only rating number\n",
        "    else:\n",
        "        rating = 'N/A'\n",
        "    ratings.append(rating)\n",
        "\n",
        "    # Extract seller name if available (Amazon pages often lack this detail in the listings)\n",
        "    seller = product.find('span', class_='a-size-base-plus')\n",
        "    if seller:\n",
        "        seller = seller.text.strip()\n",
        "    else:\n",
        "        seller = 'N/A'\n",
        "    sellers.append(seller)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "data = {\n",
        "    'Product Name': product_names,\n",
        "    'Price': prices,\n",
        "    'Rating': ratings,\n",
        "    'Seller Name': sellers\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('amazon_products.csv', index=False)\n",
        "\n",
        "print(\"Scraping completed and saved to amazon_products.csv\")\n"
      ]
    }
  ]
}